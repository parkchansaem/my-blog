---
title: 5/16
date: '2022-05-16T22:40:32.169Z'
description: 공부내역
---

ML workflow
데이터 분석 문제 정의: 이슈 파악&문제 도출, 분석 데이터 정의, 머신 러닝 문제 정의, 베이스 라인 선정
데이터 수집 및 정제: 데이터 마트 생성, 데이터 정합성 평가, 데이터 취합, 데이터 포맷 통일
탐색적 데이터 분석: 결측치 처리, 클래스 불균형 문제, 데이터 시각화, 통계량 분석, 차원의 저주, 상관관계 분석, 피처 추출, 범주형 변수 인코딩
예측 모델 개발 및 적용: 예측 모델 학습, 모델 성능 개선, 성능 평가, 최종 모델 배포
데이터 정의-> 데이터 정형화-> EDA->전처리-> ML model-> 평가

회귀(Regression)

-   회귀의 직관적인 의미
    -   주어진 데이터(X)와 원하는 값(y)사이의 관계를 찾는 방법
    -   주어진 데이터(X)를 통해서 원하는 값(y=target value)을 예측하는 방법

Linear Regression : 가장 직관적이고 많이 사용되는 선형 회귀 모델

MSE:평균제곱오차로 선형회귀에서 중심이 되는 자료 오차의 제곱을 평균으로 나눈것으로 0에 가까울수록 추측한 값이 원본에 가까운 것이기 때문에 정확도가 높다고 할수 있고 예측값과 실제값 차이의 면적을 평균이라고 할수 있다.

Linear regression이 안좋은 예측 결과가 나오면 feature engineering을 통해 새롭게 뽑아서 사용

LightGBM regressor: 실제 데이터 분석 대회에서 가장 많이 사용하는 효과적인 회귀 모델

-   여러 Decision Tree중에 target value를 잘찾는 tree들만 찾아서 그 방향으로 트리를 확장해 나감
-   대용량 데이터에 대해서 적은 메모리로도 빠르게 성능이 좋은 회귀 모델을 만들 수 있다.
-   Hyper-parameter에 영향을 많이 받기 때문에 parameter tuning이 중요함
-   기존에 많이 쓰는 파라미터 세팅을 기억해두고 필요에 따라 다양한 조합을 테스트해봅니다.
-   AutoML,pycaret

Decision tree(CART)->Random forest->Gradient boosting model(GBM)->XGBoost-> Light GBM-> Catboost

대다수가 같은 모델을 사용하여 보통 feature engining이나 하이퍼 파라미터에서 차이가 남

회귀 모델 평가

-   주어진 데이터로 모델을 학습시키는 것은 지정한 성능 평가 지표를 향상시키는 과정
-   성능 평가 지표의 값은 예측 성능을 기준으로 한다.
-   정량적 기준을 설정하고, 달성할 때까지 모델을 학습시키고 성능을 개선
-   목표한 성능에 도달한 모델을 실제 서비스에 적용
-   MSE(Mean Squared Error), RMSLE(Root Mean Squared log error):yi,yihat의 scale 영향을 안받음-outlier에 robust함(영향을 덜 받음) MAE(Mean Absolute Error), R2 Score(Coefficient of Determination)

비모수적 기법 vs 모수적 기법

-   비모수적 기법: 모집단의 확률 분포에 대한 특별한 가정없이, 평균과 분산이라는 지표를 추정하는 것
-   모수적 기법: 모집단의 성질에 따라 어떤 확률 분포의 형태인지를 미리 가정한 후, 기대값 혹은 분산을 결정하는 소수의 파라미터를 추정하는 것

통계 계산 함수

-   E(x): 확률변수의 기대값을 계산하는 함수
-   V(x): 확률변ㅅ의 분산을 계산하는 함수
-   check_prob: 확률변수의 성질을 만족하는 지 검정 후 기대값, 분산을 반환하는 함수
-   plot_prob: 확률변수의 확률밀도함수 및 기대값을 시각화하기 위한 함수

베르누이 분포

-   가장 기본적인 이산형 확률 분포
-   취할 수 있는 값은 0혹은 1
-   1과 0이 나올 확률을 각각 더하면 1이 되어야 하므로, 그 확률을 각각 p와 1-p로 정의

이항 분포

-   성공 확률이 p인 베르누이 시행을 n번 했을 때의 성공 횟수를 나타내는 분포
-   파라미터: 성공확률 p, 시행횟수 n
-   0<p<1, n은 1이상인 정수

기하 분포

-   베르누이 시행에서 처음 성공할때까지 반복한 시행횟수에 따른 분포

푸아송 분포

-   임의의 사건이 단위 시간당 발생하는 건수에 따른 확률분포
-   파리미터:람다는 양의 실수

ADSP

탐색적 자료분석(EDA):데이터가 가지고 있는 특성을 파악하기 위해 시각화하여 분석하는 방식, 시각화하면 이상점을 식별하기 쉽다.
EDA의 4가지 주제: 저항성의 강조,잔차, 계산, 자료변수의 재표현 그래프를 통한 현시성

기술 통계: 모집단으로부터 표본을 추출하고 표본이 가지고 있는 정보를 하나의 숫자 또는 그래프의 형태로 표현

추측 통계: 모집단으로붙 추출된 표본의 표본통계량으로 모집단을 통계적으로 추론

데이터마이닝 모델링

-   대표적인 고급 데이터 분석법이다(ex:시뮬레이션)
-   지나치게 통계적 가설이나 유의성에 집착하지마 말고 훈련 및 테스트 성능에 큰 편차가 없고 예상 성능을 만족하면 중단한다. 반드시 다양한 옵션을 주어야 하는 것이 아니다.

R기초

-   벡터 생성 c: c(1,2,3)은 [1,2,3]이고 숫자형 벡터이지만 c(1,2,’a’)처럼 문자형이 원소로 하나라도 껴있으면 문자형 벡터가 됨. c(1:5)는 1부터 5까지라는 뜻:를 ~로 생각
-   패키지 설치 및 로드: install.packages(“패키지명”)->library(패키지명)
-   행렬을 as.vector함수에 입력하면 열방향으로 1열부터 원소를 나열하는 벡터 생성
-   Summary 함수는 4분위수, 최소, 최대, 중앙값, 평균을 출력함
-   데이터프레임: 2차원 목록 데이터 구조, 각 열이 다른 데이터 타입을 가질 수 있다. 데이터 테이블이란 데이터 프레임과 유사하지만 보다 월등히 빠른 그루핑과 ordering, 짧은 문장 지원측 면에서 더 매력적

데이터 마트

-   데이터 웨어하우스와 사용자 사이의 중간층에 위치
-   CRM관련 업무 중에서 핵심

요약변수

-   수집된 정보를 분석에 맞게 종합한 변수, 재활용성이 높다.

파생변수

-   특정조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수
-   주관적일 수 있어 논리적 타당성을 갖추어야 한다.
    Sqldf:R에서 sql명령어를 사용 가능하게 해주는 패키지
    Plyr: apply 함수를 기반으로 가장 필수적인 데이터 처리기능 제공

결측값(na) 처리 방법

-   Completes analysis:결측값이 존재하는 레코드 삭제
-   평균 대처법: 결측값을 데이터의 평균으로 대치
-   다중 대치법: 대치, 분석, 결합 단계로 진행

이상값

-   잘못 입력한 경우, 의도하지 않게 입력되었고 분석 목적에 맞지 않는 경우
-   꼭 제거해야하는 것은 아니므로 적절한 판단 필요

이상값 인식 방법 3가지

-   ESD: 평균으로부터 3 표준편차 떨어진 값
-   기하평균 - 2.5 * 표준편차< data<기하평균+2.5*표준편차를 벗어나는 값
-   사분위수 이용하기: Q1-1.5*(IQR)<data<Q3+1.5*(IQR)을 벗어나는 값, IQR=Q3-Q1, 보통 이러한 범위를 넘어가면 제거하지 않고 범위의 최대 최소값으로 바꾸어 활용
-   사기탐지, 의료, 부정사용방지 등등에 쓰일 수 있다.

통계자료 획득

-   전수조사: 전체를 다 조사하는 것 ->시간과 비용이 많이 소요
-   표본조사: 일부만 추출하여 모집단을 분석

표본 추출 방법

-   단순랜덤 추출법: 말 그대로 랜덤하게 표본을 뽑음
-   계통추출법: 번호를 랜덤하게 부여한 후 특정한 간격별로 추출
-   집락추출법: 군집을 나눈 후 군집별로 단순랜덤 추출
-   층화추출법: 계층을 고루 대표할 수 있도록 표본 추출

표본 오차 & 표본 편의

-   표본 오차는 모집단을 대표할 수 있는 표본 단위들이 조사대상으로 추출되지 못함으로서 발생하는 오차
-   표본편의는 모수를 작게 또는 크게 할 때 추정하는 것과 같이 표본추출방법에서 기인하는 오차를 의미, 확률화로 최소화하거나 없앨 수 있다.

척도 구분

-   명목척도: 어느 집단에 속하는지 분류
-   순서척도: 서열관계가 있을때
-   구간척도: 속성의 양을 측정하는 것으로 구간이나 구간 사이의 간격이 의미가 있는 자료, 절대적 원점은 없다.
-   비율척도: 절대적 기준인 0이 존재하고 사칙연산이 가능

확률변수

-   이산형: 0이 아닌 확률값을 셀 수 있는 실수값-> 이산형 확률 변수로는 베르누이,이항,기하,다항,푸아송 분포가 있음
-   연속형: 확률이 함수형태로 주어져 있다고 생각하면 됨, 균일분포, 정규분포,지수분포,t-분포,카이제곱분포,f-분포가 있음

추정: 표본으로부터 모수을 추측하는 것

-   점추정: 모수가 특정한 값일 것이라고 추정하는 것
-   구간측정: 모수가 특정한 구간에 있을 것이라고 선언하는 것, 추정량의 분포에 대한 전제가 주어져야 하고 구해진 구간 안에 모수가 있을 신뢰구간이 주어져야한다.

가설검정

-   귀무가설: 비교하는 값과 차이가 없다를 기본개념으로 하는 가설
-   대립가설: 뚜렷한 증거가 있을 때 주장하는 가설
-   p값: 귀무가설이 사실일 때, 관측된 검정통계량이 대립가설을 지지하는 방향으로 나올 확률, 우리가 내린 판정이 잘못되었을 실제 확률을 의미
-   유의수준: 귀무가설이 옳은데도 이를 기가하는 확률의 크기
-   제 1종 오류: 귀무가설이 옳은데 귀무가설을 기가하게 되는 오류
-   제 2종 오류: 귀무가설이 틀린데 귀무가설을 채택하는 오류

비모수적 방법

-   자료가 추출된 모집단의 분포에 아무 제약 않고 검정 실시
-   분포의 형태에 대해 설정
-   절대적인 크기에 의존하지 않는 관측값들의 순위나 두 관측값 차이의 부호 등을 이용
-   부호검정, 윌콕슨의 순위합검정, 윌콕슨이 부호순위합검정, 만-위트니의 u검정, 스피어만 순위상관계수

왜도: 분포의 비대칭 정도를 나타내는 측도(평균 중앙값 최빈값 관계 그림으로 기억)

상관분석: 데이터 안의 두 변수간의 관계를 알아보기 위한 것

-   상관계수의 절대값이 0이면 전혀 상관이 없는 것이고 0.3보다 작으면 거의 상관이 없는 것 절대값이 0.3과 0.7사이라면 약한 상관관계가 있는 것 1과 0.7사이라면 강한 상관이 있는 것
-   cor() 혹은 rcorr()함수로 상관계수를 구할 수 있다.
-   R로 상관분석을 했을 때, p-value값이 0.05이하인 경우 귀무가설을 기각하고 대립가설을 채택할 수 있다. 즉 변수 간 상관관계가 있다고 볼 수 있다.

회귀분석: 하나 또는 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정하는 통계법

-   P-값이 0.05보다 작으면 추정된 회귀식은 통계적으로 유의수준 5%이하에서 유의미하다고 봄
-   결정계수는 R-squared값으로 확인하면 되고 1에 가까울수록 설명력이 높다고 판단할 수 있다. 총 변동 중에서 회귀모형에 의하여 설명되는 변동이 차지하는 비율이다.
-   선형회귀분석의 가정
    -   선형성: 입력변수와 출력변수의 관계가 선형이다.
    -   등분산성: 오차의 분산이 입력변수와 무관하게 일정하다.
    -   독립성: 입력변수와 오차는 관련이 없다.
    -   비상관성: 오차들끼리 상관이 없다.
    -   정상성: 오차의 분포가 정규분포를 따른다.
-   데이터의 정규성은 Q-Q plot, Shapiro-walks test, 히스토그램을 사용해 확인 가능

단계적 변수선택

-   전진선택법: 절편만 있는 상수모형으로부터 시작해 중요하다고 생각된ㄴ 변수를 차례대로 추가
-   후진제거법: 모든 변수를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 하나씩 제거
-   단계선택법: 전진선택법으로 변수를 추가하는데 기존 변수가 영향을 받아 중요도가 약화되면 변수를 다시 제거하는 등 단계별로 추가, 제거 여부를 검토하는 방법
-   최적회귀방정식은 모든 후보 모형들에 대해 AIC,BIC를 계산하고 그 값이 최소가 되는 모형 선택

시계열 자료: 시간의 흐름에 따라 관찰된 값을 뜻함
정상시계열(일반적으로 분산이 시점에 의존하지 않음을 나타냄)

-   모든 시점에 대해 일정한 평균과 분산을 가진다.
-   특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값을 갖는다.

비정상시계열을 정상시계열로 전환하는 방법

-   평균이 일정하지 않은 경우: 원시계열에 차분(현 시점에서 바로 전 시점의 자료값을 뺌)
-   계절성을 갖는 경우: 계절처분 사용
-   분산이 일정하지 않은 경우: 자연로그를 취함

자기회귀 모형(AR)

-   시계열 모델 중 자기 자신의 과거 값을 사용하여 설명하는 모형
-   백색 잡음의 현재값과 자기 자신의 과거값의 선형 가중합으로 이루어진 정상 확률 모형

분해시계열: 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법

-   경향요인, 계절요인, 순환요인, 불규칙요인으로 이루어짐
-   경향은 말 그대로 자료가 오르거나 내리는 추세를 의미
-   계절은 고정된 주기에 따라 자료가 변하는 경우
-   순환은 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 갖고 변화
-   불규칙은 위 3가지로 설명할 수 없을 때

다차원척도법(MDS)

-   객체간 근접성을 시각화하는 통계기법
-   개체들을 2차원 또는 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석방법
-   계량적MDS-> 비울척도,구간척도 데이터 활용
-   비계량적MDS-> 순서척도 데이터 활용

주성분분석(PCA)

-   여러 변수들을 상관관계를 이용해 소수의 주성분으로 차원을 축소하는 것
-   Scree plot을 이용하는 경우에는 그래프의 기울기가 완만해져 거의 0에 다다르는 지점에서 주성분의 개수를 구한다.
-   대략 85%의 분산설명력을 갖게끔 주성분의 수를 결정한다.

관측치 수 = 자유도(df)+1

데이터마이닝

-   대용량 데이터에서 의미있는 데이터 패턴을 파악, 예측하여 의사결정에 활용하는 방법
-   가설이나 가정없이 다양한 수리 알고리즘을 이용
-   데이터 마이닝 도구가 다양하고 체계화되어 환경에 적합한 제품을 선택하여 활용 가능
-   알고리즘에 대한 깊은 이해가 없어도 분석에 큰 어려움이 없다.
-   분석 결과의 품질을 위해서 풍분한 경험을 가진 전문가가 하면 좋다.
    지도학습:인공신경망, 의사결정나무, 회귀분석, 로지스틱회귀분석, 사례기반추론
    비지도학습: OLAP,연관성 규칙발견, 군집분석, SOM

데이터마이닝 추진 단계

-   목적설정->데이터 준비-> 가공-> 기법적용-> 검증

데이터 분할

-   구축용: 데이터 마이닝 모델을 만드는데 활용하며 보통 50%사용
-   검정용: 구축된 모향의 과대추정 또는 과소추정을 미세 조정을 하는데 활용 보통 30%사용
-   시험용: 모델의 성능을 검증하는데 활용 보통 20%사용
-   홀드아웃 방법: 주어진 데이터를 학습용과 시험용 데이터로 분리하여 사용하는 방법
-   교차확인 방법: 주어진 데이터를 K개의 집단으로 구분하여 k-1개의 집단을 학습용으로, 나머지는 검증용으로 설정해 학습하는 방법

성과분석

-   1번은 실제로도 참인데 예측도 참이라고 한 경우
-   2번은 실제로는 거짓인데 예측을 참이라고 한 경우
-   3번은 실제로 참인데 예측을 거짓이라고 한경우
-   4번은 실제로도 거잣인데 예측도 거짓이라고 한경우
-   Acuracy=(1+4)/(1+2+3+4) 실제와 맞게 예측한 확률
-   특이도(Specificity)=4/(2+4) 실제로 거짓인 사건을 예측도 거짓으로 한 확률이다.
-   정확도(Precision)= 1/(1+2) 예측을 참이라고 했는데 실제로도 참일 확률이다.
-   재현율(Recall)=민감도(Sensitivity)=1/(1+3) 실제로 참인 경우 중에 예측을 참으로 한 확률

F1 score = 2*(p*R)/(p+R)
ROC Curve: 가로축을 1- 특이도 세로축을 민감도 값으로 두어 시각화한 그래프, 그래프의 면적이 클수록(1에 가까울수록) 모형의 성능이 좋다고 평가

분류분석

-   데이터가 어떤 그룹에 속하는지 예측하는데 사용되는 기법
-   지도학습에 해당
-   인공신경망, 의사결정나무, 회귀분석 등등 위에서 다뤘던 지도학습의 대부분이 분류분석에 속함
-   향상동 곡선: 분류 분석의 모형평가 방법으로 랜덤 모델과 비교하여 해당 모델의 성과가 얼마나 향상되었는지를 각 등급별로 파악

로지스틱 회귀분석

-   반응변수가 범주형인 경우에 적용되는 회귀분석 모형
-   오즈가 함께 증가, 성공할 확률이 실패할 확률의 몇배인지를 나타냄

의사결정나무

-   연속적으로 발생하는 의사결정 문제를 시각화해 의사결정이 이뤄지는 시점과 성과를 한눈에 볼 수 있게 한다. 해석이 간편하다.
-   예측력에 치중할 때도 있고 해석력에 치중할 때도 있다.
-   대용량 데이터에서도 빠르다.
-   비정상 잡음 데이터에 대해서도 민감함이 없다
-   상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않는다.-> 조건이 맞지 않는 변수는 그냥 버리면 되니 크게 영향을 안받을 것
-   새로운 자료에 대한 과대적합이 발생할 수 있다. -> 과대적합: 너무 자세하게 만들어서 다른 자료에 적용할 때 성능이 떨어짐, 과소적합: 모형이 너무 단순해서 성능이 떨어짐
-   아래로 내려갈수록 각 마디에서의 불순도는 감소한다.

형성과정: 성장(분리규칙,정지규칙)-> 가지치기(과대적합되어 현실문제에 적응할 수 있는 적절한 규칙이 나오지 않는 현상 방지)->타당성평가(이익도표,위험도표)-> 해석 및 예측

불순도 측도: 카이제곱 통계량, 지니지수, 엔트로피 지수

의사결정나무 알고리즘

-   CART:가장 많이 할용되는 알고리즘, 출력변수가 범주형일 경우 지니지수, 연속형일 경우 이진 분리 사용
-   C4.5/C5.0: 범주형 입력변수에 대해서는 범주의 수만큼 분리가 일어남, 측도로는 엔트리피지수 사용
-   CHAID: 가지치기를 없이 적당한 크기에서 나무 성장을 중지, 입력변수는 반드시 범주형 사용, 측도로는 카이 제곱 통계량 사용

앙상블 분석: 여러개의 예측 모형들을 만든 후 조합

-   배깅: 여러개의 부스트랩 자료를 생성한 후 각 자료에 예측모형을 만든 후 결합, 가지치기를 하지않고 최대로 성장한 의사결정나무 활용
-   부스팅: 배깅과 다른 점은 각 자료에 동일한 가중치를 주는 것이 아닌 분류가 잘못된 데이터에 더 큰 가중을 준다.
-   랜덤포레스트: 배깅에 랜덤과정을 추가한 방법
-   샘플에 한번도 선택되지 않는 원데이터가 발생할 수 있는데 전체 샘플의 36.8%가 해당

인공신경망

-   인간의 뇌를 기반으로 한 추론모델
-   역전판 알고리즘을 활용해 비선형성 극복한 모형 등장-> 연결강도를 갱신하기 위해 예측된 결과와 실제값 차이인 error로 가중치 조절
-   활성화함수를 사용해 출력을 결정(입력변수의 속성에 따라 선택하는 것은 아님)-> 시그모이드 함수라는 것이 있는데 0~1의 확률값을 갖으며 로지스틱 회귀분석과 유사
-   Softmax 함수: 출력 값이 여러개로 주어지고 목표치가 다범주인 경우
-   은닉노드의 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 적용하는 것이 좋음-> 은닉층의 뉴런개수는 자동으로 설정되는 것이 아닌 직접 설정해야함. 너무 많이 설정하면 과대적합 너무 적으면 과소적합 일어남(기울기 소실문제)

군집분석

-   군집의 개수나 구조에 대한 가정없이 데이터들 사이의 거리를 기준으로 군집화 유도-> 인공신경망은 개수를 직접 설정해줘야했음
-   각 개체의 유사성을 측정하여 분류하고 서로 다른 군집의 객체들과의 상이성을 규명하는 분석방법

실루엣: 군집분석의 품질을 정량적으로 평가하는 대표적인 지표, 군집 내 데이터간 거리가 짧을수록 군집 간 거리가 멀수로 값이 커짐

연속형 변수 거리

-   유클리디안 거리: 우리가 흔히 아는 좌표계에서의 거리
-   표준화 거리: 표준화하게되면 척도, 분산의 차이로 인한 왜곡을 피할 수 있다.
-   마할라노비스 거리: 통계적 개념 포함, 변수의 표준화와 상관성을 동시에 고려
-   맨하탄거리, 민코우스키 거리

범주형 변수 거리

-   자카드 유사도: Boolean 속성으로 이루어진 두 객체간의 유사도 측정에 사용된다.
-   코사인 유사도: 두 단위 벡터의 내적을 이용, 내각의 크기로 유사도를 측정

계층적 군집 분석

-   최단 연결법: 최단거리를 이용해 군집형성, 고립된 군집을 찾는데 중점, 사슬모양의 군집이 생길 수 있음
-   최장연결법: 최장거리를 이용해 군집형성, 내부 응집성에 중점을 둠
-   중심연결법: 중심간의 거리를 이용해 군집형성
-   평균연결법: 계산량이 많지만 모든 데이터를 포함하는 하나의 군집형성
-   와드 연결법: 군집내의 오차 제곡합에 기초하여 군집 형성

비계층적 군집 분석(K 평균 군집분석)

-   원하는 군집의 개수와 초기값들을 정해 seed를 중심으로 군집을 형성-> 집단 내 제곱합 그래프를 참고해 군집수를 정할 수 있음(어려움)
-   한번 군집이 형성되어도 개체들은 다른 군집으로 이동할 수 있음
-   잡음이나 이상값에 영향을 많이 받음
-   주어진 목적이 없어 결과 해석이 어렵다.(극복하기 위해 PAM 방법 사용)
-   볼록한 형태가 아닌 군집이 존재할 경우 성능이 떨어짐
-   내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다는 장점이 있다.

흔한분포군집: EM알고리즘이 사용된다.

SOM(자기조직화지도)

-   입력변수의 위치 관계를 그대로 보존한다는 특정이 있다.
-   역전파 알고리즘을 사용하는 인공신경망과 달리 단 하나의 전방패스를 사용
-   연결강도는 입력패턴과 가장 유사한 경쟁층 뉴런이 승자가 된다.
-   BMU:SOM에서 선택된 프로토타입 벡터
-   입력 변수의 개수와 동일하게 뉴런 수가 존재
-   고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬

연관분석
